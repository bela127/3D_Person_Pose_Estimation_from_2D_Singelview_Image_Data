{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Obtaining file:///tf/pose3D/src\n",
      "Installing collected packages: ShAReD-Net\n",
      "  Attempting uninstall: ShAReD-Net\n",
      "    Found existing installation: ShAReD-Net 1.0\n",
      "    Uninstalling ShAReD-Net-1.0:\n",
      "      Successfully uninstalled ShAReD-Net-1.0\n",
      "  Running setup.py develop for ShAReD-Net\n",
      "Successfully installed ShAReD-Net\n",
      "\u001b[33mWARNING: The directory '/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: addict in /.local/lib/python3.6/site-packages (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -e ../\n",
    "!{sys.executable} -m pip install addict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]\n",
      "0\n",
      "0\n",
      "12\n",
      "12\n",
      "4 Physical GPUs, 4 Logical GPUs\n",
      "physical_devs [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'), PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'), PhysicalDevice(name='/physical_device:XLA_GPU:1', device_type='XLA_GPU'), PhysicalDevice(name='/physical_device:XLA_GPU:2', device_type='XLA_GPU'), PhysicalDevice(name='/physical_device:XLA_GPU:3', device_type='XLA_GPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]\n",
      "logical_devs [LogicalDevice(name='/device:CPU:0', device_type='CPU'), LogicalDevice(name='/device:XLA_CPU:0', device_type='XLA_CPU'), LogicalDevice(name='/device:XLA_GPU:0', device_type='XLA_GPU'), LogicalDevice(name='/device:XLA_GPU:1', device_type='XLA_GPU'), LogicalDevice(name='/device:XLA_GPU:2', device_type='XLA_GPU'), LogicalDevice(name='/device:XLA_GPU:3', device_type='XLA_GPU'), LogicalDevice(name='/device:GPU:0', device_type='GPU'), LogicalDevice(name='/device:GPU:1', device_type='GPU'), LogicalDevice(name='/device:GPU:2', device_type='GPU'), LogicalDevice(name='/device:GPU:3', device_type='GPU')]\n",
      "2.2.0-dev20200403\n"
     ]
    }
   ],
   "source": [
    "def init_devs():\n",
    "    tf.config.set_soft_device_placement(True)\n",
    "    \n",
    "    options = {\n",
    "                \"layout_optimizer\": True,\n",
    "                \"constant_folding\": True,\n",
    "                \"shape_optimization\": True,\n",
    "                \"remapping\": True,\n",
    "                \"arithmetic_optimization\": True,\n",
    "                \"dependency_optimization\": True,\n",
    "                \"loop_optimization\": True,\n",
    "                \"function_optimization\": True,\n",
    "                \"debug_stripper\": True,\n",
    "                \"disable_model_pruning\": False,\n",
    "                \"scoped_allocator_optimization\": True,\n",
    "                \"pin_to_host_optimization\": True,\n",
    "                \"implementation_selector\": True,\n",
    "                \"disable_meta_optimizer\": False\n",
    "              }\n",
    "    #tf.config.optimizer.set_experimental_options(options)\n",
    "\n",
    "    \n",
    "    devs = tf.config.get_visible_devices()\n",
    "    print(devs)\n",
    "\n",
    "    print(tf.config.threading.get_inter_op_parallelism_threads())\n",
    "    print(tf.config.threading.get_intra_op_parallelism_threads())\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(12)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(12)\n",
    "    print(tf.config.threading.get_inter_op_parallelism_threads())\n",
    "    print(tf.config.threading.get_intra_op_parallelism_threads())\n",
    "\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    gpus = gpus[:] \n",
    "    if gpus:\n",
    "        try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            tf.config.experimental.set_visible_devices(gpus, 'GPU')\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e)\n",
    "    logical_devs = tf.config.list_logical_devices()\n",
    "    physical_devs = tf.config.experimental.list_physical_devices()\n",
    "\n",
    "    print(\"physical_devs\",physical_devs)\n",
    "    print(\"logical_devs\", logical_devs)\n",
    "    \n",
    "    print(tf.version.VERSION)\n",
    "init_devs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ShAReD_Net.training.train_distributed as train\n",
    "\n",
    "import ShAReD_Net.training.slim as training_slim\n",
    "import ShAReD_Net.model.slim as model_slim\n",
    "import ShAReD_Net.training.loss.slim as loss_slim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ShAReD_Net.configure import config\n",
    "\n",
    "config.dataset.IMG_PATH = \"/dataset/jta/images\"\n",
    "config.dataset.ANNO_PATH = \"/dataset/jta/new_image_annotations\"\n",
    "\n",
    "config.checkpoint.path = \"/tf/pose3D/checkpoints/run6\"\n",
    "config.tensorboard.path = \"/tf/pose3D/logdir/run6\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ShAReD_Net.data.transform.transform as transform\n",
    "\n",
    "data_split = \"train\"\n",
    "\n",
    "def create_dataset(per_replica_batch_size):\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        train_ds = transform.create_dataset(data_split, 4).shuffle(500).prefetch(100)\n",
    "    return train_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loss():\n",
    "    loss = training_slim.SlimTrainingLoss()\n",
    "    return loss\n",
    "    \n",
    "def train_model():\n",
    "    \n",
    "    low_level_extractor = model_slim.LowLevelExtractor(color_channel=13, texture_channel=16, texture_compositions=16, out_channel=32)\n",
    "\n",
    "    encoder = model_slim.Encoder(dense_blocks_count=3, dense_filter_count=8)\n",
    "    \n",
    "    pos_decoder = model_slim.PosDecoder(dense_blocks_count=2, dense_filter_count=8)\n",
    "    \n",
    "    pose_decoder = model_slim.PoseDecoder(keypoints=config.model.output.keypoints, z_bins=config.model.z_bins, dense_blocks_count=3, dense_filter_count=8)\n",
    "    \n",
    "    model = training_slim.SlimTrainingModel(low_level_extractor, encoder, pos_decoder, pose_decoder)    \n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_checkpoint(step, optimizer, train_model, train_loss):\n",
    "    nets = {\"low_level_extractor\":train_model.low_level_extractor,\n",
    "            \"encoder\":train_model.encoder,\n",
    "            \"pos_decoder\":train_model.pos_decoder,\n",
    "            \"pose_decoder\":train_model.pose_decoder,\n",
    "            \"loss_agg\":train_loss.loss_agg,\n",
    "           }\n",
    "    ckpt = tf.train.Checkpoint(step=step, optimizer=optimizer, **nets)\n",
    "    manager = tf.train.CheckpointManager(ckpt, config.checkpoint.path, max_to_keep=50)\n",
    "    return ckpt, manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ShAReD_Net.model.modules.slim as slim_modules\n",
    "\n",
    "roi_size_img = np.asarray(config.model.roi_size) * config.model.img_downsampling + 1\n",
    "        \n",
    "roi_extractor = slim_modules.Roi_Extractor(roi_size=roi_size_img)\n",
    "\n",
    "def calc_img_index(roiindices):\n",
    "    new_indexes = roiindices * config.model.img_downsampling\n",
    "    return new_indexes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss(dev, step, batch, output, loss, extra_loss, ckpt, manager, train_model, grads):\n",
    "    loss_per_batch, detection_loss_sum, (loss_pos_xy_sum, loss_var_xy_sum), (loss_pos_z_sum, loss_var_z_sum) = loss    \n",
    "    \n",
    "    extra_loss_sum = tf.reduce_sum(extra_loss) / 10\n",
    "       \n",
    "    tf.print(\"On\", dev)\n",
    "    tf.print(\"detection_loss\", detection_loss_sum)\n",
    "    \n",
    "    tf.print(\"estimator_loss_xy\", loss_pos_xy_sum)\n",
    "    tf.print(\"estimator_loss_z\", loss_pos_z_sum)\n",
    "    \n",
    "    tf.print(\"estimator_loss_var_xy\", loss_var_xy_sum)\n",
    "    tf.print(\"estimator_loss_var_z\", loss_var_z_sum)\n",
    "    \n",
    "    tf.print(\"extra_loss_sum\", extra_loss_sum)\n",
    "\n",
    "\n",
    "def simple_summery(dev, step, batch, output, loss, extra_loss, ckpt, manager, train_model, grads):\n",
    "    loss_per_batch, detection_loss_sum, (loss_pos_xy_sum, loss_var_xy_sum), (loss_pos_z_sum, loss_var_z_sum) = loss    \n",
    "    poses_xyz, pos_hm, (pose_prob_map_xy, pose_prob_maps_z) = output\n",
    "    img, (pos_hm_gt, loss_weights), roi_indexes, (pose_xyz_gt, pose_indexes) = batch\n",
    "    tf.print(\"simple_summery\")  \n",
    "    \n",
    "    loss_per_batch_sum = tf.reduce_sum(loss_per_batch)\n",
    "    \n",
    "    extra_loss_sum = tf.reduce_sum(extra_loss) / 10\n",
    "    \n",
    "    tf.summary.scalar(f\"detection_loss\", detection_loss_sum)\n",
    "    \n",
    "    tf.summary.scalar(f\"loss_pos_xy_sum\", loss_pos_xy_sum)\n",
    "    tf.summary.scalar(f\"loss_pos_z_sum\", loss_pos_z_sum)\n",
    "    \n",
    "    tf.summary.scalar(f\"loss_var_xy_sum\", loss_var_xy_sum)\n",
    "    tf.summary.scalar(f\"loss_var_z_sum\", loss_var_z_sum)\n",
    "    \n",
    "    tf.summary.scalar(f\"extra_loss\", extra_loss_sum)\n",
    "    \n",
    "    tf.summary.scalar(f\"agg_loss\", loss_per_batch_sum)\n",
    "    \n",
    "    mean_grad = tf.zeros(())\n",
    "    for grad in grads:\n",
    "        mean_grad += tf.reduce_mean(tf.abs(grad))\n",
    "    mean_grad /= len(grads)\n",
    "    \n",
    "    max_grad = tf.zeros(())\n",
    "    for grad in grads:\n",
    "        max_grad = tf.math.maximum(tf.reduce_max(tf.abs(grad)),max_grad)\n",
    "        \n",
    "    tf.summary.scalar(f\"max_grad\", max_grad)\n",
    "    \n",
    "    tf.summary.scalar(f\"mean_grad\", mean_grad)\n",
    "    \n",
    "    for k in range(config.model.output.keypoints):\n",
    "        tf.summary.scalar(f\"pose x, kp {k}\", tf.reduce_mean(tf.abs(poses_xyz[:,k,0]-pose_xyz_gt[:,k,0])))\n",
    "        tf.summary.scalar(f\"pose y, kp {k}\", tf.reduce_mean(tf.abs(poses_xyz[:,k,0]-pose_xyz_gt[:,k,1])))\n",
    "        tf.summary.scalar(f\"pose z, kp {k}\", tf.reduce_mean(tf.abs(poses_xyz[:,k,0]-pose_xyz_gt[:,k,2])))\n",
    "    \n",
    "    \n",
    "def complex_summery(dev, step, batch, output, loss, extra_loss, ckpt, manager, train_model, grads):\n",
    "    poses_xyz, pos_hm, (pose_prob_map_xy, pose_prob_maps_z) = output\n",
    "    img, (pos_hm_gt, loss_weights), roi_indexes, (pose_xyz_gt, pose_indexes) = batch\n",
    "    tf.print(\"complex_summery\")\n",
    "    \n",
    "    tf.summary.image(\"image\", img, max_outputs=4)\n",
    "    \n",
    "    pos_hm_gt_near = pos_hm_gt[...,0,None]\n",
    "    pos_hm_gt_fare = pos_hm_gt[...,1,None]\n",
    "    tf.summary.image(\"pos_hm_gt_near\", pos_hm_gt_near, max_outputs=4)\n",
    "    tf.summary.image(\"pos_hm_gt_fare\", pos_hm_gt_fare, max_outputs=4)\n",
    "    \n",
    "    pos_hm_near = pos_hm[...,0,None]\n",
    "    pos_hm_fare = pos_hm[...,1,None]\n",
    "    tf.summary.image(\"pos_hm_near\", pos_hm_near, max_outputs=4)\n",
    "    tf.summary.image(\"pos_hm_fare\", pos_hm_fare, max_outputs=4)\n",
    "    \n",
    "    new_indexes = calc_img_index(roi_indexes)\n",
    "    pose_imges = roi_extractor([img, new_indexes])\n",
    "    tf.summary.image(\"pose_imges\", pose_imges, max_outputs=4)\n",
    "    tf.summary.image(f\"pose_hm\", tf.reduce_sum(pose_prob_map_xy,axis=1), max_outputs=4)\n",
    "    \n",
    "    pose_prob_map_xy = tf.unstack(pose_prob_map_xy, axis=1)\n",
    "    for i, pose_hm in enumerate(pose_prob_map_xy):\n",
    "        tf.summary.image(f\"pose_hm for keypoint {i}\", pose_hm, max_outputs=2)\n",
    "    \n",
    "    pose_prob_maps_z = tf.unstack(pose_prob_maps_z, axis=1)\n",
    "    for i, z_slice in enumerate(pose_prob_maps_z):\n",
    "        tf.summary.image(f\"z_slice for keypoint {i}\", z_slice[...,None,None], max_outputs=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(dev, step, batch, output, loss, extra_loss, ckpt, manager, train_model, grads):\n",
    "    def save():\n",
    "        save_path = manager.save()\n",
    "        print(\"Saved checkpoint for step {}: {}\".format(int(ckpt.step), save_path))\n",
    "    tf.py_function(save,[], [])\n",
    "\n",
    "def finalize(dev, step, batch, output, loss, extra_loss, ckpt, manager, train_model, grads):\n",
    "    def run():\n",
    "        print(\"Finalized\")\n",
    "        tf.Graph.finalize(tf.compat.v1.get_default_graph())\n",
    "    tf.py_function(run,[], [])\n",
    "\n",
    "\n",
    "def count_params(dev, step, batch, output, loss, extra_loss, ckpt, manager, train_model, grads):\n",
    "    print(train_model.count_params())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batching(dataset, batch_size):\n",
    "    batched_ds = dataset.batch(batch_size)\n",
    "\n",
    "    def unragg(img, pos_stuff, roi_indexes, pose_stuff):\n",
    "        rel_pose, pose_indexes = pose_stuff\n",
    "        pos_heatmap, weights = pos_stuff\n",
    "\n",
    "        def unragg_roi_indexes(indexes, row_length):\n",
    "            new_indexes = np.empty([indexes.shape[0], indexes.shape[1]+1], dtype=np.int32)\n",
    "            new_indexes[:,1:] = indexes\n",
    "            i = 0\n",
    "            b = 0\n",
    "            for length in row_length[1]:\n",
    "                index = indexes[i:int(i+length)]\n",
    "                new_indexes[i:int(i+length),0] = b\n",
    "                i += length\n",
    "                b += 1\n",
    "                \n",
    "            return new_indexes\n",
    "        \n",
    "        def unragg_pose_indexes(indexes, row_length):\n",
    "            new_indexes = np.empty([indexes.shape[0], indexes.shape[1], indexes.shape[-1]+1], dtype=np.int32)\n",
    "            new_indexes[:,:,1:] = indexes\n",
    "            i = 0\n",
    "            b = 0\n",
    "            for length in row_length[1]:\n",
    "                index = indexes[i:int(i+length)]\n",
    "                new_indexes[i:int(i+length),:,0] = b\n",
    "                i += length\n",
    "                b += 1\n",
    "                \n",
    "            return new_indexes\n",
    "\n",
    "        roi_indexes_flat = tf.numpy_function(unragg_roi_indexes, [roi_indexes.flat_values, roi_indexes.nested_row_lengths()], Tout=roi_indexes.dtype)\n",
    "        pose_indexes_flat = tf.numpy_function(unragg_pose_indexes, [pose_indexes.flat_values, pose_indexes.nested_row_lengths()], Tout=pose_indexes.dtype)\n",
    "\n",
    "        rel_pose_flat =  rel_pose.flat_values\n",
    "\n",
    "        return img, (pos_heatmap, weights), roi_indexes_flat, (rel_pose_flat, pose_indexes_flat)\n",
    "\n",
    "    unragged_ds = batched_ds.map(unragg).prefetch(100)\n",
    "    return unragged_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_pre(batch):\n",
    "    img, (pos_heatmap, weights), roi_indexes, (rel_pose, pose_indexes) = batch\n",
    "    return img, roi_indexes, pose_indexes\n",
    "    \n",
    "def loss_pre(output, batch):\n",
    "    poses_xyz, pos_hm, (pose_prob_map_xy, pose_prob_maps_z) = output\n",
    "    img, (pos_hm_gt, loss_weights), roi_indexes, (pose_xyz_gt, pose_indexes) = batch\n",
    "    return (poses_xyz, pose_xyz_gt, pose_prob_map_xy, pose_prob_maps_z), (pos_hm, pos_hm_gt, loss_weights)\n",
    "\n",
    "def grad_pre(loss, extra_loss, batch, optimizer, train_model, train_loss):\n",
    "    loss_per_batch, detection_loss_sum, (loss_pos_xy_sum, loss_var_xy_sum), (loss_pos_z_sum, loss_var_z_sum) = loss    \n",
    "    \n",
    "    extra_loss_sum = tf.reduce_sum(extra_loss) / 100\n",
    "    \n",
    "    loss_per_batch_sum = loss_per_batch\n",
    "        \n",
    "    trainable_vars =  train_model.low_level_extractor.trainable_variables + train_model.encoder.trainable_variables + train_model.pos_decoder.trainable_variables + train_model.pose_decoder.trainable_variables+ train_loss.trainable_variables\n",
    "        \n",
    "    return loss_per_batch, extra_loss_sum, trainable_vars\n",
    "\n",
    "\n",
    "#grad_norm = loss_slim.GradNormLayer()\n",
    "\n",
    "def opt_pre(loss, extra_loss, batch, optimizer, train_model, train_loss):\n",
    "    loss_per_batch, detection_loss_sum, (loss_pos_xy_sum, loss_var_xy_sum), (loss_pos_z_sum, loss_var_z_sum) = loss    \n",
    "    \n",
    "    extra_loss_sum = tf.reduce_sum(extra_loss) / 10\n",
    "    \n",
    "    detector = loss_per_batch[0]\n",
    "    pose = loss_per_batch[1]\n",
    "    var = train_model.encoder.stage3.trainable_variables\n",
    "    \n",
    "    detector_grad = optimizer.get_gradients(detector, var)\n",
    "    pose_grad = optimizer.get_gradients(pose, var)\n",
    "    weighted_loss, weighting_loss = grad_norm([loss_per_batch, (detector_grad, pose_grad)])\n",
    "    \n",
    "    agg_loss = weighted_loss + weighting_loss\n",
    "    \n",
    "    trainable_vars = train_model.low_level_extractor.trainable_variables + train_model.encoder.trainable_variables + train_model.pos_decoder.trainable_variables + train_model.pose_decoder.trainable_variables+ train_loss.trainable_variables\n",
    "    return agg_loss, extra_loss, trainable_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    }
   ],
   "source": [
    "dist_strat = tf.distribute.MirroredStrategy(cross_device_ops = tf.distribute.HierarchicalCopyAllReduce())\n",
    "\n",
    "steps = 100000\n",
    "\n",
    "step_callbacks = train.standart_callbacks()\n",
    "step_callbacks.every_steps[20] = print_loss\n",
    "step_callbacks.every_steps[200] = save_checkpoint\n",
    "step_callbacks.every_steps[10] = simple_summery\n",
    "step_callbacks.every_steps[50] = complex_summery\n",
    "\n",
    "step_callbacks.at_step[1] = count_params\n",
    "step_callbacks.at_step[2] = finalize\n",
    "\n",
    "step_callbacks.make_batches = batching\n",
    "step_callbacks.grad_pre = grad_pre\n",
    "step_callbacks.input_pre = input_pre\n",
    "step_callbacks.loss_pre = loss_pre\n",
    "\n",
    "step_callbacks.create_dataset = create_dataset\n",
    "step_callbacks.create_ckpt = create_checkpoint\n",
    "step_callbacks.create_loss = train_loss\n",
    "step_callbacks.create_model = train_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored from /tf/pose3D/checkpoints/run6/ckpt-20\n",
      "WARNING:tensorflow:From /tf/pose3D/src/ShAReD_Net/training/train_distributed.py:181: StrategyBase.experimental_run_v2 (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "renamed to `run`\n",
      "SlimInferenzModel (TensorShape([None, None, None, 3]), TensorShape(None), TensorShape(None))\n",
      "Extractor (None, None, None, 3)\n",
      "Encoder (None, None, None, 32)\n",
      "stage1 [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 32])]\n",
      "big_shared1 [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 32])]\n",
      "scale_down (None, None, None, 32)\n",
      "ResAttention [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 32])]\n",
      "Attention [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 32])]\n",
      "DenseModule (None, None, None, 128)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "DenseBlock (None, None, None, 40)\n",
      "BnDoConfReluConfRelu (None, None, None, 40)\n",
      "scale_up (None, None, None, 48)\n",
      "big_normal (None, None, None, 48)\n",
      "normal_shared1 [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 48])]\n",
      "scale_down (None, None, None, 32)\n",
      "ResAttention [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 48])]\n",
      "Attention [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 48])]\n",
      "DenseModule (None, None, None, 128)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "DenseBlock (None, None, None, 40)\n",
      "BnDoConfReluConfRelu (None, None, None, 40)\n",
      "scale_up (None, None, None, 48)\n",
      "normal_medium (None, None, None, 48)\n",
      "medium_shared1 [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 48])]\n",
      "scale_down (None, None, None, 32)\n",
      "ResAttention [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 48])]\n",
      "Attention [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 48])]\n",
      "DenseModule (None, None, None, 128)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "DenseBlock (None, None, None, 40)\n",
      "BnDoConfReluConfRelu (None, None, None, 40)\n",
      "scale_up (None, None, None, 48)\n",
      "medium_small (None, None, None, 48)\n",
      "small_shared1 [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 48])]\n",
      "scale_down (None, None, None, 32)\n",
      "ResAttention [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 48])]\n",
      "Attention [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 48])]\n",
      "DenseModule (None, None, None, 128)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "DenseBlock (None, None, None, 40)\n",
      "BnDoConfReluConfRelu (None, None, None, 40)\n",
      "scale_up (None, None, None, 48)\n",
      "small_medium (None, None, None, 48)\n",
      "medium_shared3 [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 96])]\n",
      "scale_down (None, None, None, 32)\n",
      "ResAttention [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 96])]\n",
      "Attention [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 96])]\n",
      "DenseModule (None, None, None, 128)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "DenseBlock (None, None, None, 40)\n",
      "BnDoConfReluConfRelu (None, None, None, 40)\n",
      "scale_up (None, None, None, 48)\n",
      "medium_normal (None, None, None, 48)\n",
      "normal_shared3 [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 96])]\n",
      "scale_down (None, None, None, 32)\n",
      "ResAttention [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 96])]\n",
      "Attention [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 96])]\n",
      "DenseModule (None, None, None, 128)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "DenseBlock (None, None, None, 40)\n",
      "BnDoConfReluConfRelu (None, None, None, 40)\n",
      "scale_up (None, None, None, 48)\n",
      "normal_big (None, None, None, 48)\n",
      "big_shared3 [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 96])]\n",
      "scale_down (None, None, None, 32)\n",
      "ResAttention [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 96])]\n",
      "Attention [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 96])]\n",
      "DenseModule (None, None, None, 128)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "DenseBlock (None, None, None, 40)\n",
      "BnDoConfReluConfRelu (None, None, None, 40)\n",
      "scale_up (None, None, None, 48)\n",
      "scale_1_2_res (None, None, None, 32)\n",
      "scale_1_2_shc (None, None, None, 48)\n",
      "stage2 [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 48])]\n",
      "big_shared1 [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 48])]\n",
      "scale_down (None, None, None, 32)\n",
      "ResAttention [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 48])]\n",
      "Attention [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 48])]\n",
      "DenseModule (None, None, None, 128)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "DenseBlock (None, None, None, 40)\n",
      "BnDoConfReluConfRelu (None, None, None, 40)\n",
      "scale_up (None, None, None, 48)\n",
      "big_normal (None, None, None, 48)\n",
      "normal_shared1 [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 48])]\n",
      "scale_down (None, None, None, 32)\n",
      "ResAttention [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 48])]\n",
      "Attention [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 48])]\n",
      "DenseModule (None, None, None, 128)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "DenseBlock (None, None, None, 40)\n",
      "BnDoConfReluConfRelu (None, None, None, 40)\n",
      "scale_up (None, None, None, 48)\n",
      "normal_medium (None, None, None, 48)\n",
      "medium_shared1 [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 48])]\n",
      "scale_down (None, None, None, 32)\n",
      "ResAttention [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 48])]\n",
      "Attention [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 48])]\n",
      "DenseModule (None, None, None, 128)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "DenseBlock (None, None, None, 40)\n",
      "BnDoConfReluConfRelu (None, None, None, 40)\n",
      "scale_up (None, None, None, 48)\n",
      "medium_small (None, None, None, 48)\n",
      "small_shared1 [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 48])]\n",
      "scale_down (None, None, None, 32)\n",
      "ResAttention [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 48])]\n",
      "Attention [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 48])]\n",
      "DenseModule (None, None, None, 128)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "DenseBlock (None, None, None, 40)\n",
      "BnDoConfReluConfRelu (None, None, None, 40)\n",
      "scale_up (None, None, None, 48)\n",
      "small_medium (None, None, None, 48)\n",
      "medium_shared3 [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 96])]\n",
      "scale_down (None, None, None, 32)\n",
      "ResAttention [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 96])]\n",
      "Attention [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 96])]\n",
      "DenseModule (None, None, None, 128)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "DenseBlock (None, None, None, 40)\n",
      "BnDoConfReluConfRelu (None, None, None, 40)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_up (None, None, None, 48)\n",
      "medium_normal (None, None, None, 48)\n",
      "normal_shared3 [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 96])]\n",
      "scale_down (None, None, None, 32)\n",
      "ResAttention [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 96])]\n",
      "Attention [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 96])]\n",
      "DenseModule (None, None, None, 128)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "DenseBlock (None, None, None, 40)\n",
      "BnDoConfReluConfRelu (None, None, None, 40)\n",
      "scale_up (None, None, None, 48)\n",
      "normal_big (None, None, None, 48)\n",
      "big_shared3 [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 96])]\n",
      "scale_down (None, None, None, 32)\n",
      "ResAttention [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 96])]\n",
      "Attention [TensorShape([None, None, None, 32]), TensorShape([None, None, None, 96])]\n",
      "DenseModule (None, None, None, 128)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "DenseBlock (None, None, None, 40)\n",
      "BnDoConfReluConfRelu (None, None, None, 40)\n",
      "scale_up (None, None, None, 48)\n",
      "scale_1_4_res (None, None, None, 32)\n",
      "scale_1_4_shc (None, None, None, 48)\n",
      "stage3 [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 48])]\n",
      "big_shared1 [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 48])]\n",
      "scale_down (None, None, None, 64)\n",
      "ResAttention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 48])]\n",
      "Attention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 48])]\n",
      "DenseModule (None, None, None, 256)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "DenseBlock (None, None, None, 40)\n",
      "BnDoConfReluConfRelu (None, None, None, 40)\n",
      "scale_up (None, None, None, 48)\n",
      "big_normal (None, None, None, 48)\n",
      "normal_shared1 [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 48])]\n",
      "scale_down (None, None, None, 64)\n",
      "ResAttention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 48])]\n",
      "Attention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 48])]\n",
      "DenseModule (None, None, None, 256)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "DenseBlock (None, None, None, 40)\n",
      "BnDoConfReluConfRelu (None, None, None, 40)\n",
      "scale_up (None, None, None, 48)\n",
      "normal_medium (None, None, None, 48)\n",
      "medium_shared1 [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 48])]\n",
      "scale_down (None, None, None, 64)\n",
      "ResAttention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 48])]\n",
      "Attention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 48])]\n",
      "DenseModule (None, None, None, 256)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "DenseBlock (None, None, None, 40)\n",
      "BnDoConfReluConfRelu (None, None, None, 40)\n",
      "scale_up (None, None, None, 48)\n",
      "medium_small (None, None, None, 48)\n",
      "small_shared1 [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 48])]\n",
      "scale_down (None, None, None, 64)\n",
      "ResAttention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 48])]\n",
      "Attention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 48])]\n",
      "DenseModule (None, None, None, 256)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "DenseBlock (None, None, None, 40)\n",
      "BnDoConfReluConfRelu (None, None, None, 40)\n",
      "scale_up (None, None, None, 48)\n",
      "small_medium (None, None, None, 48)\n",
      "medium_shared3 [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 96])]\n",
      "scale_down (None, None, None, 64)\n",
      "ResAttention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 96])]\n",
      "Attention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 96])]\n",
      "DenseModule (None, None, None, 256)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "DenseBlock (None, None, None, 40)\n",
      "BnDoConfReluConfRelu (None, None, None, 40)\n",
      "scale_up (None, None, None, 48)\n",
      "medium_normal (None, None, None, 48)\n",
      "normal_shared3 [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 96])]\n",
      "scale_down (None, None, None, 64)\n",
      "ResAttention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 96])]\n",
      "Attention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 96])]\n",
      "DenseModule (None, None, None, 256)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "DenseBlock (None, None, None, 40)\n",
      "BnDoConfReluConfRelu (None, None, None, 40)\n",
      "scale_up (None, None, None, 48)\n",
      "normal_big (None, None, None, 48)\n",
      "big_shared3 [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 96])]\n",
      "scale_down (None, None, None, 64)\n",
      "ResAttention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 96])]\n",
      "Attention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 96])]\n",
      "DenseModule (None, None, None, 256)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "DenseBlock (None, None, None, 40)\n",
      "BnDoConfReluConfRelu (None, None, None, 40)\n",
      "scale_up (None, None, None, 48)\n",
      "scale_1_8_res (None, None, None, 64)\n",
      "scale_1_8_shc (None, None, None, 48)\n",
      "PosDecoder [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 48])]\n",
      "SelfShAReD [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 48])]\n",
      "ShAReD [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 48])]\n",
      "ResAttention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 48])]\n",
      "Attention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 48])]\n",
      "ShReD [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 192])]\n",
      "DenseModule (None, None, None, 256)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "Attention [TensorShape([None, None, None, 48]), TensorShape([None, None, None, 40])]\n",
      "ShAReD [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 96])]\n",
      "ResAttention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 96])]\n",
      "Attention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 96])]\n",
      "ShReD [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 192])]\n",
      "DenseModule (None, None, None, 256)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "ShAReDHourGlass [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 40])]\n",
      "big_shared1 [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 40])]\n",
      "scale_down (None, None, None, 64)\n",
      "ResAttention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 40])]\n",
      "Attention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 40])]\n",
      "DenseModule (None, None, None, 256)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "scale_up (None, None, None, 40)\n",
      "big_normal (None, None, None, 40)\n",
      "normal_shared1 [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 40])]\n",
      "scale_down (None, None, None, 64)\n",
      "ResAttention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 40])]\n",
      "Attention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 40])]\n",
      "DenseModule (None, None, None, 256)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "scale_up (None, None, None, 40)\n",
      "normal_medium (None, None, None, 40)\n",
      "medium_shared1 [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 40])]\n",
      "scale_down (None, None, None, 64)\n",
      "ResAttention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 40])]\n",
      "Attention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 40])]\n",
      "DenseModule (None, None, None, 256)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "scale_up (None, None, None, 40)\n",
      "medium_small (None, None, None, 40)\n",
      "small_shared1 [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 40])]\n",
      "scale_down (None, None, None, 64)\n",
      "ResAttention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 40])]\n",
      "Attention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 40])]\n",
      "DenseModule (None, None, None, 256)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "scale_up (None, None, None, 40)\n",
      "small_medium (None, None, None, 40)\n",
      "medium_shared3 [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 80])]\n",
      "scale_down (None, None, None, 64)\n",
      "ResAttention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 80])]\n",
      "Attention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 80])]\n",
      "DenseModule (None, None, None, 256)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "scale_up (None, None, None, 40)\n",
      "medium_normal (None, None, None, 40)\n",
      "normal_shared3 [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 80])]\n",
      "scale_down (None, None, None, 64)\n",
      "ResAttention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 80])]\n",
      "Attention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 80])]\n",
      "DenseModule (None, None, None, 256)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "scale_up (None, None, None, 40)\n",
      "normal_big (None, None, None, 40)\n",
      "big_shared3 [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 80])]\n",
      "scale_down (None, None, None, 64)\n",
      "ResAttention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 80])]\n",
      "Attention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 80])]\n",
      "DenseModule (None, None, None, 256)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "scale_up (None, None, None, 40)\n",
      "Scale (None, None, None, 64)\n",
      "Scale (None, None, None, 40)\n",
      "SelfShAReD [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 40])]\n",
      "ShAReD [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 40])]\n",
      "ResAttention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 40])]\n",
      "Attention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 40])]\n",
      "ShReD [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 192])]\n",
      "DenseModule (None, None, None, 256)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "Attention [TensorShape([None, None, None, 40]), TensorShape([None, None, None, 40])]\n",
      "ShAReD [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 80])]\n",
      "ResAttention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 80])]\n",
      "Attention [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 80])]\n",
      "ShReD [TensorShape([None, None, None, 64]), TensorShape([None, None, None, 192])]\n",
      "DenseModule (None, None, None, 256)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "Roi_Extractor [TensorShape([None, None, None, 112]), TensorShape([None, 3])]\n",
      "PosDecoder (None, 31, 31, 112)\n",
      "SelfShAReD [TensorShape([None, 31, 31, 112]), TensorShape([None, 31, 31, 112])]\n",
      "ShAReD [TensorShape([None, 31, 31, 112]), TensorShape([None, 31, 31, 112])]\n",
      "ResAttention [TensorShape([None, 31, 31, 112]), TensorShape([None, 31, 31, 112])]\n",
      "Attention [TensorShape([None, 31, 31, 112]), TensorShape([None, 31, 31, 112])]\n",
      "ShReD [TensorShape([None, 31, 31, 112]), TensorShape([None, 31, 31, 336])]\n",
      "DenseModule (None, 31, 31, 448)\n",
      "DenseBlock (None, 31, 31, 24)\n",
      "BnDoConfReluConfRelu (None, 31, 31, 24)\n",
      "DenseBlock (None, 31, 31, 32)\n",
      "BnDoConfReluConfRelu (None, 31, 31, 32)\n",
      "DenseBlock (None, 31, 31, 40)\n",
      "BnDoConfReluConfRelu (None, 31, 31, 40)\n",
      "Attention [TensorShape([None, 31, 31, 112]), TensorShape([None, 31, 31, 48])]\n",
      "ShAReD [TensorShape([None, 31, 31, 112]), TensorShape([None, 31, 31, 224])]\n",
      "ResAttention [TensorShape([None, 31, 31, 112]), TensorShape([None, 31, 31, 224])]\n",
      "Attention [TensorShape([None, 31, 31, 112]), TensorShape([None, 31, 31, 224])]\n",
      "ShReD [TensorShape([None, 31, 31, 112]), TensorShape([None, 31, 31, 336])]\n",
      "DenseModule (None, 31, 31, 448)\n",
      "DenseBlock (None, 31, 31, 24)\n",
      "BnDoConfReluConfRelu (None, 31, 31, 24)\n",
      "DenseBlock (None, 31, 31, 32)\n",
      "BnDoConfReluConfRelu (None, 31, 31, 32)\n",
      "DenseBlock (None, 31, 31, 40)\n",
      "BnDoConfReluConfRelu (None, 31, 31, 40)\n",
      "ShAReDHourGlass [TensorShape([None, 31, 31, 112]), TensorShape([None, 31, 31, 48])]\n",
      "big_shared1 [TensorShape([None, 31, 31, 112]), TensorShape([None, 31, 31, 48])]\n",
      "scale_down (None, 31, 31, 112)\n",
      "ResAttention [TensorShape([None, None, None, 112]), TensorShape([None, 31, 31, 48])]\n",
      "Attention [TensorShape([None, None, None, 112]), TensorShape([None, 31, 31, 48])]\n",
      "DenseModule (None, 31, 31, 448)\n",
      "DenseBlock (None, 31, 31, 24)\n",
      "BnDoConfReluConfRelu (None, 31, 31, 24)\n",
      "DenseBlock (None, 31, 31, 32)\n",
      "BnDoConfReluConfRelu (None, 31, 31, 32)\n",
      "DenseBlock (None, 31, 31, 40)\n",
      "BnDoConfReluConfRelu (None, 31, 31, 40)\n",
      "scale_up (None, 31, 31, 48)\n",
      "big_normal (None, 31, 31, 48)\n",
      "normal_shared1 [TensorShape([None, 31, 31, 112]), TensorShape([None, None, None, 48])]\n",
      "scale_down (None, 31, 31, 112)\n",
      "ResAttention [TensorShape([None, None, None, 112]), TensorShape([None, None, None, 48])]\n",
      "Attention [TensorShape([None, None, None, 112]), TensorShape([None, None, None, 48])]\n",
      "DenseModule (None, None, None, 448)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "DenseBlock (None, None, None, 40)\n",
      "BnDoConfReluConfRelu (None, None, None, 40)\n",
      "scale_up (None, None, None, 48)\n",
      "normal_medium (None, None, None, 48)\n",
      "medium_shared1 [TensorShape([None, 31, 31, 112]), TensorShape([None, None, None, 48])]\n",
      "scale_down (None, 31, 31, 112)\n",
      "ResAttention [TensorShape([None, None, None, 112]), TensorShape([None, None, None, 48])]\n",
      "Attention [TensorShape([None, None, None, 112]), TensorShape([None, None, None, 48])]\n",
      "DenseModule (None, None, None, 448)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "DenseBlock (None, None, None, 40)\n",
      "BnDoConfReluConfRelu (None, None, None, 40)\n",
      "scale_up (None, None, None, 48)\n",
      "medium_small (None, None, None, 48)\n",
      "small_shared1 [TensorShape([None, 31, 31, 112]), TensorShape([None, None, None, 48])]\n",
      "scale_down (None, 31, 31, 112)\n",
      "ResAttention [TensorShape([None, None, None, 112]), TensorShape([None, None, None, 48])]\n",
      "Attention [TensorShape([None, None, None, 112]), TensorShape([None, None, None, 48])]\n",
      "DenseModule (None, None, None, 448)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "DenseBlock (None, None, None, 40)\n",
      "BnDoConfReluConfRelu (None, None, None, 40)\n",
      "scale_up (None, None, None, 48)\n",
      "small_medium (None, None, None, 48)\n",
      "medium_shared3 [TensorShape([None, 31, 31, 112]), TensorShape([None, None, None, 96])]\n",
      "scale_down (None, 31, 31, 112)\n",
      "ResAttention [TensorShape([None, None, None, 112]), TensorShape([None, None, None, 96])]\n",
      "Attention [TensorShape([None, None, None, 112]), TensorShape([None, None, None, 96])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseModule (None, None, None, 448)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "DenseBlock (None, None, None, 40)\n",
      "BnDoConfReluConfRelu (None, None, None, 40)\n",
      "scale_up (None, None, None, 48)\n",
      "medium_normal (None, None, None, 48)\n",
      "normal_shared3 [TensorShape([None, 31, 31, 112]), TensorShape([None, None, None, 96])]\n",
      "scale_down (None, 31, 31, 112)\n",
      "ResAttention [TensorShape([None, None, None, 112]), TensorShape([None, None, None, 96])]\n",
      "Attention [TensorShape([None, None, None, 112]), TensorShape([None, None, None, 96])]\n",
      "DenseModule (None, None, None, 448)\n",
      "DenseBlock (None, None, None, 24)\n",
      "BnDoConfReluConfRelu (None, None, None, 24)\n",
      "DenseBlock (None, None, None, 32)\n",
      "BnDoConfReluConfRelu (None, None, None, 32)\n",
      "DenseBlock (None, None, None, 40)\n",
      "BnDoConfReluConfRelu (None, None, None, 40)\n",
      "scale_up (None, None, None, 48)\n",
      "normal_big (None, None, None, 48)\n",
      "big_shared3 [TensorShape([None, 31, 31, 112]), TensorShape([None, 31, 31, 96])]\n",
      "scale_down (None, 31, 31, 112)\n",
      "ResAttention [TensorShape([None, None, None, 112]), TensorShape([None, 31, 31, 96])]\n",
      "Attention [TensorShape([None, None, None, 112]), TensorShape([None, 31, 31, 96])]\n",
      "DenseModule (None, 31, 31, 448)\n",
      "DenseBlock (None, 31, 31, 24)\n",
      "BnDoConfReluConfRelu (None, 31, 31, 24)\n",
      "DenseBlock (None, 31, 31, 32)\n",
      "BnDoConfReluConfRelu (None, 31, 31, 32)\n",
      "DenseBlock (None, 31, 31, 40)\n",
      "BnDoConfReluConfRelu (None, 31, 31, 40)\n",
      "scale_up (None, 31, 31, 48)\n",
      "PositionDependency (None, 31, 31, 70)\n",
      "SelfShAReD [TensorShape([None, 31, 31, 73]), TensorShape([None, 31, 31, 48])]\n",
      "ShAReD [TensorShape([None, 31, 31, 73]), TensorShape([None, 31, 31, 48])]\n",
      "ResAttention [TensorShape([None, 31, 31, 73]), TensorShape([None, 31, 31, 48])]\n",
      "Attention [TensorShape([None, 31, 31, 73]), TensorShape([None, 31, 31, 48])]\n",
      "ShReD [TensorShape([None, 31, 31, 73]), TensorShape([None, 31, 31, 219])]\n",
      "DenseModule (None, 31, 31, 292)\n",
      "DenseBlock (None, 31, 31, 24)\n",
      "BnDoConfReluConfRelu (None, 31, 31, 24)\n",
      "DenseBlock (None, 31, 31, 32)\n",
      "BnDoConfReluConfRelu (None, 31, 31, 32)\n",
      "DenseBlock (None, 31, 31, 40)\n",
      "BnDoConfReluConfRelu (None, 31, 31, 40)\n",
      "Attention [TensorShape([None, 31, 31, 48]), TensorShape([None, 31, 31, 48])]\n",
      "ShAReD [TensorShape([None, 31, 31, 73]), TensorShape([None, 31, 31, 96])]\n",
      "ResAttention [TensorShape([None, 31, 31, 73]), TensorShape([None, 31, 31, 96])]\n",
      "Attention [TensorShape([None, 31, 31, 73]), TensorShape([None, 31, 31, 96])]\n",
      "ShReD [TensorShape([None, 31, 31, 73]), TensorShape([None, 31, 31, 219])]\n",
      "DenseModule (None, 31, 31, 292)\n",
      "DenseBlock (None, 31, 31, 24)\n",
      "BnDoConfReluConfRelu (None, 31, 31, 24)\n",
      "DenseBlock (None, 31, 31, 32)\n",
      "BnDoConfReluConfRelu (None, 31, 31, 32)\n",
      "DenseBlock (None, 31, 31, 40)\n",
      "BnDoConfReluConfRelu (None, 31, 31, 40)\n",
      "SlimTrainPoseExtractor [TensorShape([None, 31, 31, 35]), TensorShape([None, 15, 3])]\n",
      "SlimInferenzModel (None, 31, 31, 15)\n",
      "LocationMap None\n",
      "SlimInferenzModel [TensorShape([None, 31, 31, 20]), TensorShape([None, 15, 3])]\n",
      "SlimTrainingLoss ((TensorShape([None, 15, 3]), TensorShape([None, 15, 3]), TensorShape([None, 15, 31, 31, 1]), TensorShape([None, 15, 20])), (TensorShape([None, None, None, 2]), TensorShape([None, None, None, 2]), TensorShape([None, None, None, 2])))\n",
      "DetectionLoss [TensorShape([None, None, None, 2]), (TensorShape([None, None, None, 2]), TensorShape([None, None, None, 2]))]\n",
      "PoseLoss [TensorShape([None, 15, 3]), TensorShape([None, 15, 3]), TensorShape([None, 15, 31, 31, 1]), TensorShape([None, 15, 20])]\n",
      "PoseLoss2D [TensorShape([None, 15, 2]), TensorShape([None, 15, 2]), TensorShape([None, 15, 31, 31, 1])]\n",
      "LocationMap None\n",
      "PoseLossDepth [TensorShape([None, 15, 1]), TensorShape([None, 15, 1]), TensorShape([None, 15, 20])]\n",
      "LossAggregation [TensorShape([None]), (TensorShape([None, 15, 2]), TensorShape([None, 15, 2])), (TensorShape([None, 15, 1]), TensorShape([None, 15, 1]))]\n",
      "MultiLossLayer [TensorShape([]), TensorShape([]), TensorShape([])]\n",
      "tracing gradients on [['/job:localhost/replica:0/task:0/device:GPU:0']]\n",
      "tracing gradients on [['/job:localhost/replica:0/task:0/device:GPU:1']]\n",
      "tracing gradients on [['/job:localhost/replica:0/task:0/device:GPU:2']]\n",
      "tracing gradients on [['/job:localhost/replica:0/task:0/device:GPU:3']]\n",
      "INFO:tensorflow:batch_all_reduce: 1667 all-reduces with algorithm = hierarchical_copy, num_packs = 1\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:574: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "Roi_Extractor [TensorShape([None, None, None, 3]), TensorShape(None)]\n",
      "4946826\n",
      "4946826\n",
      "4946826\n",
      "4946826\n",
      "tracing gradients on [['/job:localhost/replica:0/task:0/device:GPU:0']]\n",
      "tracing gradients on [['/job:localhost/replica:0/task:0/device:GPU:1']]\n",
      "tracing gradients on [['/job:localhost/replica:0/task:0/device:GPU:2']]\n",
      "tracing gradients on [['/job:localhost/replica:0/task:0/device:GPU:3']]\n",
      "INFO:tensorflow:batch_all_reduce: 1667 all-reduces with algorithm = hierarchical_copy, num_packs = 1\n",
      "4946826\n",
      "4946826\n",
      "4946826\n",
      "4946826\n",
      "Step: 1001\n",
      "Step: 1001\n",
      "Step: 1001\n",
      "Step: 1001\n",
      "Step: 1002\n",
      "Step: 1002\n",
      "Step: 1002\n",
      "Step: 1002\n",
      "Step: 1003\n",
      "Step: 1003\n",
      "Step: 1003\n",
      "Step: 1003\n",
      "Step: 1004\n",
      "Step: 1004\n",
      "Step: 1004\n",
      "Step: 1004\n",
      "Step: 1005\n",
      "Step: 1005\n",
      "Step: 1005\n",
      "Step: 1005\n",
      "Step: 1006\n",
      "Step: 1006\n",
      "Step: 1006\n",
      "Step: 1006\n",
      "Step: 1007\n",
      "Step: 1007\n",
      "Step: 1007\n",
      "Step: 1007\n",
      "Step: 1008\n",
      "Step: 1008\n",
      "Step: 1008\n",
      "Step: 1008\n",
      "Step: 1009\n",
      "Step: 1009\n",
      "Step: 1009\n",
      "Step: 1009\n",
      "simple_summery\n",
      "Step: 1010\n",
      "Step: 1010\n",
      "Step: 1010\n",
      "Step: 1010\n",
      "simple_summery\n",
      "simple_summery\n",
      "simple_summery\n",
      "Step: 1011\n",
      "Step: 1011\n",
      "Step: 1011\n",
      "Step: 1011\n",
      "Step: 1012\n",
      "Step: 1012\n",
      "Step: 1012\n",
      "Step: 1012\n",
      "Step: 1013\n",
      "Step: 1013\n",
      "Step: 1013\n",
      "Step: 1013\n",
      "Step: 1014\n",
      "Step: 1014\n",
      "Step: 1014\n",
      "Step: 1014\n",
      "Step: 1015\n",
      "Step: 1015\n",
      "Step: 1015\n",
      "Step: 1015\n",
      "Step: 1016\n",
      "Step: 1016\n",
      "Step: 1016\n",
      "Step: 1016\n",
      "Step: 1017\n",
      "Step: 1017\n",
      "Step: 1017\n",
      "Step: 1017\n",
      "Step: 1018\n",
      "Step: 1018\n",
      "Step: 1018\n",
      "Step: 1018\n",
      "Step: 1019\n",
      "Step: 1019\n",
      "Step: 1019\n",
      "Step: 1019\n",
      "Step: 1020\n",
      "simple_summery\n",
      "Step: 1020\n",
      "Step: 1020\n",
      "Step: 1020\n",
      "On [['/job:localhost/replica:0/task:0/device:GPU:0']]\n",
      "detection_loss 0.721512139\n",
      "estimator_loss_xy 21883.0566\n",
      "estimator_loss_z 14.3618546\n",
      "estimator_loss_var_xy 18271.666\n",
      "estimator_loss_var_z 15.1448174\n",
      "extra_loss_sum 0.00528926542\n",
      "On [['/job:localhost/replica:0/task:0/device:GPU:1']]\n",
      "detection_loss 0.698527575\n",
      "estimator_loss_xy 14154.0674\n",
      "estimator_loss_z 1.70331216\n",
      "estimator_loss_var_xy 12459.7324\n",
      "estimator_loss_var_z 0.61889708\n",
      "extra_loss_sum 0.00528926542\n",
      "On [['/job:localhost/replica:0/task:0/device:GPU:2']]\n",
      "detection_loss 0.666624665\n",
      "estimator_loss_xy 43526.6328\n",
      "estimator_loss_z 27.7518616\n",
      "estimator_loss_var_xy 38611.8359\n",
      "estimator_loss_var_z 24.8900852\n",
      "extra_loss_sum 0.00528926542\n",
      "On [['/job:localhost/replica:0/task:0/device:GPU:3']]\n",
      "detection_loss 0.920710742\n",
      "estimator_loss_xy 16651.1133\n",
      "estimator_loss_z 3.12374067\n",
      "estimator_loss_var_xy 14318.2441\n",
      "estimator_loss_var_z 1.18240404\n",
      "extra_loss_sum 0.00528926542\n",
      "simple_summery\n",
      "simple_summery\n",
      "simple_summery\n",
      "Step: 1021\n",
      "Step: 1021\n",
      "Step: 1021\n",
      "Step: 1021\n",
      "Step: 1022\n",
      "Step: 1022\n",
      "Step: 1022\n",
      "Step: 1022\n",
      "Step: 1023\n",
      "Step: 1023\n",
      "Step: 1023\n",
      "Step: 1023\n",
      "Step: 1024\n",
      "Step: 1024\n",
      "Step: 1024\n",
      "Step: 1024\n",
      "Step: 1025\n",
      "Step: 1025\n",
      "Step: 1025\n",
      "Step: 1025\n",
      "Step: 1026\n",
      "Step: 1026\n",
      "Step: 1026\n",
      "Step: 1026\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "2 root error(s) found.\n  (0) Invalid argument:  Expected image (JPEG, PNG, or GIF), got empty file\n\t [[{{node DecodePng}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[while/body/_53/while/IteratorGetNextAsOptional_2]]\n\t [[while/body/_53/while/StatefulPartitionedCall/StatefulPartitionedCall/Nadam/Nadam/update_1168/update_3/AssignVariableOp_1/_110190]]\n  (1) Invalid argument:  Expected image (JPEG, PNG, or GIF), got empty file\n\t [[{{node DecodePng}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[while/body/_53/while/IteratorGetNextAsOptional_2]]\n0 successful operations.\n3 derived errors ignored. [Op:__inference_train_loop_1785124]\n\nFunction call stack:\ntrain_loop -> train_loop\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2068edbf972a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_strat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~tf/pose3D/src/ShAReD_Net/training/train_distributed.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(steps, dist_strat, batch_size, learning_rate, callbacks)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    676\u001b[0m               *args, **kwds)\n\u001b[1;32m    677\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_stateful_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument:  Expected image (JPEG, PNG, or GIF), got empty file\n\t [[{{node DecodePng}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[while/body/_53/while/IteratorGetNextAsOptional_2]]\n\t [[while/body/_53/while/StatefulPartitionedCall/StatefulPartitionedCall/Nadam/Nadam/update_1168/update_3/AssignVariableOp_1/_110190]]\n  (1) Invalid argument:  Expected image (JPEG, PNG, or GIF), got empty file\n\t [[{{node DecodePng}}]]\n\t [[MultiDeviceIteratorGetNextFromShard]]\n\t [[RemoteCall]]\n\t [[while/body/_53/while/IteratorGetNextAsOptional_2]]\n0 successful operations.\n3 derived errors ignored. [Op:__inference_train_loop_1785124]\n\nFunction call stack:\ntrain_loop -> train_loop\n"
     ]
    }
   ],
   "source": [
    "train.train(steps, dist_strat, batch_size = config.training.batch_size, learning_rate=config.training.learning_rate, callbacks = step_callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "estimator_loss_z 228960.422\n",
    "estimator_loss_xy 15820.8545\n",
    "\n",
    "detection_loss 98.055069\n",
    "\n",
    "\n",
    "estimator_loss_var_z 15143.8232\n",
    "estimator_loss_var_xy 124911\n",
    "\n",
    "cp 606"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
